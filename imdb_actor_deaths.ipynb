{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOU9fPzxW5sLG7Mttu4enx2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeCraftIA/imdb-dead-actors-scraper/blob/main/imdb_actor_deaths.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pM79chKR83x",
        "outputId": "3a6446fb-07dd-4dd7-ed60-f3ba8eae0ef8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xlsxwriter\n",
            "  Downloading XlsxWriter-3.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Downloading XlsxWriter-3.2.0-py3-none-any.whl (159 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-3.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install xlsxwriter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from logging import exception\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "def extract_lines(input_file_path, start_line, end_line, output_file_path):\n",
        "    try:\n",
        "        with open(input_file_path, 'r') as input_file, open(output_file_path, 'w') as output_file:\n",
        "            lines = input_file.readlines()\n",
        "            # Adjusting start_line and end_line to be within the valid range\n",
        "            start_line = max(1, min(start_line, len(lines)))\n",
        "            end_line = max(start_line, min(end_line, len(lines)))\n",
        "\n",
        "            # Writing the selected lines to the output file\n",
        "            output_file.writelines(lines[start_line - 1:end_line])\n",
        "\n",
        "        print(f\"Lines {start_line} to {end_line} extracted and saved to {output_file_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "\n",
        "def scrape_data(url, name):\n",
        "    # Define the custom headers\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36\"\n",
        "    }\n",
        "    response = requests.get(url, headers=headers)\n",
        "\n",
        "    if not response.ok:\n",
        "        print('Status Code:', response.status_code)\n",
        "        raise Exception('Failed to fetch: ', url)\n",
        "\n",
        "    soup = BeautifulSoup(response.text)\n",
        "\n",
        "    # Initialize variables to store the extracted information\n",
        "    born_date = \"\"\n",
        "    born_location = \"\"\n",
        "    death_date = \"\"\n",
        "    death_cause = \"\"\n",
        "    personal_det = soup.find('section', {'data-testid': 'PersonalDetails'})\n",
        "    if personal_det:\n",
        "      lis = personal_det.find_all('li', class_='ipc-metadata-list__item')\n",
        "      # Iterate over each list item to find the relevant details\n",
        "      for li in lis:\n",
        "          tag = li.find('span', class_='ipc-metadata-list-item__label')\n",
        "          if tag:\n",
        "            tag = tag.text.strip()\n",
        "          if tag == 'Born':\n",
        "              # Extract the born date\n",
        "              born_info = li.find_all('a')\n",
        "              try:\n",
        "                born_date = f\"{born_info[0].text}, {born_info[1].text}\"  # e.g., \"December 9, 1929\"\n",
        "              except Exception as e:\n",
        "                born_date =\"\"\n",
        "              try:\n",
        "                # Extract the born location\n",
        "                born_location = born_info[2].text  # This assumes the location is the third <a> tag\n",
        "              except Exception as e:\n",
        "                born_location =\"\"\n",
        "\n",
        "          elif tag == 'Died':\n",
        "              # Extract the death date\n",
        "              death_info = li.find_all('a')\n",
        "              try:\n",
        "                # Extract the born location\n",
        "                death_date = f\"{death_info[0].text}, {death_info[1].text}\"  # e.g., \"February 3, 1989\"\n",
        "              except Exception as e:\n",
        "                death_date =\"\"\n",
        "\n",
        "              try:\n",
        "                # Extract the cause of death\n",
        "                death_cause_span = li.find('span', class_='ipc-metadata-list-item__list-content-item--subText')\n",
        "                if death_cause_span:\n",
        "                    death_cause = death_cause_span.text.strip()\n",
        "                    death_cause = death_cause.replace('(', '').replace(')', '')\n",
        "                break\n",
        "              except Exception as e:\n",
        "                death_cause =\"\"\n",
        "\n",
        "    #Return a Dictionary\n",
        "\n",
        "    return{\n",
        "        'Name': name,\n",
        "        'IMDB URL': url,\n",
        "        'Born Date': born_date,\n",
        "        'Born Location': born_location,\n",
        "        'Death Date': death_date,\n",
        "        'Cause of Death': death_cause,\n",
        "    }\n",
        "\n",
        "\n",
        "def iterate_urls(file_path):\n",
        "    # Read URLs and names from the file\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.read().splitlines()\n",
        "\n",
        "    # Scrape data for each URL\n",
        "    results = []\n",
        "    for line in tqdm(lines):\n",
        "        try:\n",
        "            url, name = line.split(' | ')  # Split the line into URL and name\n",
        "            data = scrape_data(url, name)  # Assuming scrape_data only needs the URL\n",
        "            results.append(data)\n",
        "        except Exception as e:\n",
        "            print(f\"Error scraping data for URL: {url}\\nError: {e}\")\n",
        "            # Write the error URL and name to errors.txt\n",
        "            with open('errors_imdb.txt', 'a') as error_file:\n",
        "                error_file.write(f\"{url} | {name}\\n\")\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "i6EuntMuSahV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "def write_excel(items, path):\n",
        "    if len(items) == 0:\n",
        "        return\n",
        "\n",
        "    # Convert the list of dictionaries to a DataFrame\n",
        "    df = pd.DataFrame(items)\n",
        "\n",
        "    # Write DataFrame to Excel\n",
        "    with pd.ExcelWriter(path, engine='xlsxwriter') as writer:\n",
        "        df.to_excel(writer, index=False, sheet_name='Sheet1')"
      ],
      "metadata": {
        "id": "r05RaU6lVnU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_lines('imdb_mexico_all.txt', 1701, 3300, 'file.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmNjk4vnVpYe",
        "outputId": "75284b1b-aee6-4ff1-f1df-e7c7982f5dcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lines 1701 to 3300 extracted and saved to file.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "urls_file_path = 'file.txt'\n",
        "output_csv_path = 'imdb_all3.xlsx'\n",
        "\n",
        "# Get scraped data for each URL\n",
        "scraped_data = iterate_urls(urls_file_path)\n",
        "\n",
        "# Write the results to a CSV file\n",
        "write_excel(scraped_data, output_csv_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WgBeHCPEVy5W",
        "outputId": "02c36145-ac6a-4865-8760-3f3c2baaf739"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1600/1600 [36:39<00:00,  1.37s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the results to a CSV file\n",
        "write_excel(scraped_data, output_csv_path)"
      ],
      "metadata": {
        "id": "Cg-EJUv2WZt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wikipedia"
      ],
      "metadata": {
        "id": "KnQ4Rnd89tqh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from logging import exception\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "# Define the custom headers\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36\"\n",
        "}\n",
        "offset = 1000\n",
        "while offset < 9501:\n",
        "  url = f\"https://es.wikipedia.org/w/index.php?limit=500&offset={offset}&profile=default&search=actor+mexicano&title=Especial:Buscar&ns0=1&ns100=1&ns104=1\"\n",
        "  response = requests.get(url, headers=headers)\n",
        "  if not response.ok:\n",
        "      print('Status Code:', response.status_code)\n",
        "      raise Exception('Failed to fetch')\n",
        "\n",
        "  soup = BeautifulSoup(response.text)\n",
        "  content = soup.find('ul', class_='mw-search-results')\n",
        "  # Find all elements that match the search result headings\n",
        "  results = content.select(\"div.mw-search-result-heading a\")\n",
        "\n",
        "  # Extract the href and text (name) from each element\n",
        "  links_and_names = [(\"https://es.wikipedia.org/\" + result['href'], result.text) for result in results]\n",
        "\n",
        "  with open(\"wikipedia.txt\", 'a', encoding='utf-8') as file:\n",
        "      for link, name in links_and_names:\n",
        "          file.write(f\"{link} | {name}\\n\")\n",
        "  offset+=500\n",
        "  print(offset)\n",
        "  time.sleep(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieoqrh5DFJuz",
        "outputId": "dcedc38d-a797-4c7d-8772-a53838b78b1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1500\n",
            "2000\n",
            "2500\n",
            "3000\n",
            "3500\n",
            "4000\n",
            "4500\n",
            "5000\n",
            "5500\n",
            "6000\n",
            "6500\n",
            "7000\n",
            "7500\n",
            "8000\n",
            "8500\n",
            "9000\n",
            "9500\n",
            "10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the Excel file\n",
        "file_path = 'merged_imdb_mexico.xlsx'  # Replace with your file path\n",
        "sheet_name = 'Sheet1'  # Replace with the actual sheet name if necessary\n",
        "\n",
        "# Read the Excel file into a DataFrame\n",
        "df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
        "\n",
        "# Count non-empty rows in the \"Death Date\" column\n",
        "non_empty_count = df['Death Date'].dropna().shape[0]\n",
        "\n",
        "print(f\"Number of non-empty 'Death Date' rows: {non_empty_count}\")\n"
      ],
      "metadata": {
        "id": "tNMxwRaiIW1B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c007df0-b63e-43c1-d6e7-b2e6699de4c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of non-empty 'Death Date' rows: 2439\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the Excel file\n",
        "file_path = 'merged_imdb_mexico.xlsx'  # Replace with your file path\n",
        "sheet_name = 'Sheet1'  # Replace with the actual sheet name if necessary\n",
        "\n",
        "# Read the Excel file into a DataFrame\n",
        "df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
        "\n",
        "# Filter rows with non-empty \"Death Date\"\n",
        "non_empty_df = df[df['Death Date'].notna()]\n",
        "\n",
        "# Filter rows with empty \"Death Date\"\n",
        "empty_df = df[df['Death Date'].isna()]\n",
        "\n",
        "# Save the non-empty rows to a new Excel file\n",
        "non_empty_file_path = 'non_empty_death_date.xlsx'\n",
        "non_empty_df.to_excel(non_empty_file_path, index=False)\n",
        "\n",
        "# Save the empty rows to a new Excel file\n",
        "empty_file_path = 'empty_death_date.xlsx'\n",
        "empty_df.to_excel(empty_file_path, index=False)\n",
        "\n",
        "print(f\"Non-empty 'Death Date' rows saved to: {non_empty_file_path}\")\n",
        "print(f\"Empty 'Death Date' rows saved to: {empty_file_path}\")"
      ],
      "metadata": {
        "id": "42rcXnkF-TO4",
        "outputId": "95a9009b-403e-4ad5-ab21-b971c4960c83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Non-empty 'Death Date' rows saved to: non_empty_death_date.xlsx\n",
            "Empty 'Death Date' rows saved to: empty_death_date.xlsx\n"
          ]
        }
      ]
    }
  ]
}